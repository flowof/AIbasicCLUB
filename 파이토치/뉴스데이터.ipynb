{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYtu8NEecQBp",
    "outputId": "8558056c-b96a-4573-cc3c-0bf30d520d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PgUZLM5wdSuu",
    "outputId": "20247e61-1603-49ab-c3f4-bd5a0da8d445"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: Is: command not found\n"
     ]
    }
   ],
   "source": [
    "#!Is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H_roaXOIdUzy"
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.chdir('drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vjmq6IZYuEQe",
    "outputId": "d81209f8-c39b-4917-eb37-218aa65dc7af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\0114m\\anaconda3\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in c:\\users\\0114m\\anaconda3\\lib\\site-packages (from konlpy) (4.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\0114m\\anaconda3\\lib\\site-packages (from konlpy) (1.2.1)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\0114m\\anaconda3\\lib\\site-packages (from konlpy) (4.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\0114m\\anaconda3\\lib\\site-packages (from konlpy) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\0114m\\anaconda3\\lib\\site-packages (from konlpy) (1.16.2)\n",
      "Requirement already satisfied: tweepy>=3.7.0 in c:\\users\\0114m\\anaconda3\\lib\\site-packages (from konlpy) (3.10.0)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in c:\\users\\0114m\\anaconda3\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\0114m\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.12.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\0114m\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\users\\0114m\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (2.21.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\0114m\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\0114m\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\0114m\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\0114m\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\0114m\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in c:\\users\\0114m\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.6.8)\n"
     ]
    }
   ],
   "source": [
    "# OS 라이브러리 임포트\n",
    "import os\n",
    "# 정규표현식 라이브러리 임포트\n",
    "import re\n",
    "\n",
    "# sckit-learn 라이브러리 임포트\n",
    "from sklearn import datasets, model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 형태소 분석기 라이브러리\n",
    "!pip install konlpy\n",
    "from konlpy.tag import Hannanum\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "# pandas 라이브러리 임포트\n",
    "import pandas as pd\n",
    "\n",
    "# numpy 라이브러리 임포트\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "FUnF-aIZ0e8J",
    "outputId": "574165b1-0557-4feb-e445-c2ee211e13c1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 데이터 정리\n",
    "dir_prefix = 'C:/Users/0114m/쥬피터 소스파일/파이토치/data/'\n",
    "target_dir = 'HKIB-20000'\n",
    "cat_dirs = ['health', 'economy', 'science', 'education', 'culture', 'society', 'industry', 'leisure', 'politics']\n",
    "cat_prefixes = ['건강', '경제', '과학', '교육', '문화', '사회', '산업', '여가', '정치']\n",
    "\n",
    "files = os.listdir(dir_prefix + target_dir) # os.listdir() : 지정한 디렉토리 내의 모든 파일과 디렉토리의 리스트를 리턴함.\n",
    "\n",
    "# 5분할된 텍스트 파일을 각각 처리\n",
    "for file in files:\n",
    "\n",
    "    #데이터가 담긴 파일만 처리\n",
    "    if not file.endswith('.txt'):\n",
    "        continue\n",
    "\n",
    "    # 각 텍스트 파일을 처리\n",
    "    with open(dir_prefix + target_dir + '/' + file, 'rt', encoding='UTF8') as currfile:\n",
    "        doc_cnt = 0\n",
    "        docs = []\n",
    "        curr_doc = None\n",
    "        \n",
    "        # 기사 단위로 분할해서 리스트를 생성\n",
    "        for curr_line in currfile:\n",
    "            if curr_line.startswith('@DOCUMENT'):\n",
    "                if curr_doc is not None:\n",
    "                    docs.append(curr_doc)\n",
    "                curr_doc = curr_line\n",
    "                doc_cnt = doc_cnt + 1\n",
    "                continue\n",
    "            curr_doc = curr_doc + curr_line\n",
    "            \n",
    "        # 각 기사를 대주제별로 분류해서 기사별 파일로 정리\n",
    "        for doc in docs:\n",
    "            doc_lines = doc.split('\\n')\n",
    "            doc_no = doc_lines[1][9:]\n",
    "\n",
    "            # 주제 추출\n",
    "            doc_cat03 = ''\n",
    "            for line in doc_lines[:10]:\n",
    "                if line.startswith(\"#CAT'03:\"):\n",
    "                    doc_cat03 = line[10:]\n",
    "                    break\n",
    "            \n",
    "            # 추출한 주제별로 디렉터리 정리\n",
    "            for cat_prefix in cat_prefixes:\n",
    "                if doc_cat03.startswith(cat_prefix):\n",
    "                    dir_index = cat_prefixes.index(cat_prefix)\n",
    "                    break\n",
    "\n",
    "            # 문서 정보를 제거하고 기사 본문만 남기기\n",
    "            filtered_lines = []\n",
    "            for line in doc_lines:\n",
    "                if not (line.startswith('#') or line.startswith('@')):\n",
    "                    filtered_lines.append(line)\n",
    "\n",
    "            # 주제별 디렉터리에 기사를 파일로 쓰기\n",
    "            filename = 'hkib-' + doc_no + '.txt'\n",
    "            filepath = dir_prefix + target_dir + '/' + cat_dirs[dir_index]\n",
    "\n",
    "            if not os.path.exists(filepath):\n",
    "                os.makedirs(filepath)\n",
    "            f = open(filepath + '/' + filename, 'w', encoding='UTF8')\n",
    "            f.write('\\n'.join(filtered_lines))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "356ftDrYZVo1"
   },
   "outputs": [],
   "source": [
    "# 대상이 되는 주제 폴더 선택(경제, 사회)\n",
    "dirs = ['education', 'health']\n",
    "\n",
    "# 각 폴더의 파일을 하나씩 읽어 들임\n",
    "for i, d in enumerate(dirs):\n",
    "    # 파일 목록 읽어오기\n",
    "    files = os.listdir(dir_prefix + target_dir + '/' + d)\n",
    "    \n",
    "    for file in files:\n",
    "        # 각 파일을 읽어 들이기\n",
    "        f = open(dir_prefix + target_dir + '/' + d + '/' + file, 'r', encoding = 'utf-8')\n",
    "        raw = f.read()\n",
    "        \n",
    "        # 정규표현식을 사용해 불필요한 문자열을 제거한 다음 파일 내용을 출력\n",
    "        reg_raw = re.sub(r'[0-9a-zA-Z]', '', raw)\n",
    "        reg_raw = re.sub(r'[-\\'@#:/(<>!-\"*\\(\\)]', '', raw) # 까만 마름모 어딨지?\n",
    "        reg_raw = re.sub(r'[ ]+', ' ', reg_raw)\n",
    "        reg_raw = reg_raw.replace('\\n', ' ')\n",
    "                         \n",
    "        # 파일 닫기\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "mL61lfcAZGEl",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "java.lang.UnsatisfiedLinkError: Native Library C:\\Users\\0114m\\Anaconda3\\Lib\\site-packages\\_jpype.cp37-win_amd64.pyd already loaded in another classloader",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-6f0d204f6240>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# 형태소 분석기 객체 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# tokenizer_han = Hannanum()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKkma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_kkma.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, jvmpath, max_heap_size)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjvmpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_heap_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mjpype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misJVMStarted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mjvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_jvm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjvmpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_heap_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mkkmaJavaPackage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjpype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJPackage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'kr.lucypark.kkma'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\konlpy\\jvm.py\u001b[0m in \u001b[0;36minit_jvm\u001b[1;34m(jvmpath, max_heap_size)\u001b[0m\n\u001b[0;32m     65\u001b[0m                                 \u001b[1;34m'-Dfile.encoding=UTF8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                                 \u001b[1;34m'-ea'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-Xmx{}m'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_heap_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                                 convertStrings=True)\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Please specify the JVM path.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\jpype\\_core.py\u001b[0m in \u001b[0;36mstartJVM\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         _jpype.startup(jvmpath, tuple(args),\n\u001b[1;32m--> 222\u001b[1;33m                        ignoreUnrecognized, convertStrings, interrupt)\n\u001b[0m\u001b[0;32m    223\u001b[0m         \u001b[0minitializeResources\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: java.lang.UnsatisfiedLinkError: Native Library C:\\Users\\0114m\\Anaconda3\\Lib\\site-packages\\_jpype.cp37-win_amd64.pyd already loaded in another classloader"
     ]
    }
   ],
   "source": [
    "# 대상이 되는 주제 폴더 선택(교육, 건강)\n",
    "dirs = ['education', 'health']\n",
    "\n",
    "# 기사에 출현하는 단어와 레이블을 저장할 리스트를 생성\n",
    "# 설명변수\n",
    "x_ls = []\n",
    "# 목적변수\n",
    "y_ls = []\n",
    "\n",
    "tmp1 = []\n",
    "tmp2 = ''\n",
    "\n",
    "# 형태소 분석기 객체 생성\n",
    "# tokenizer_han = Hannanum()\n",
    "tokenizer = Kkma()\n",
    "\n",
    "\n",
    "# 각 폴더의 파일을 하나씩 읽어 들이며, 전처리 후 리스트에 저장\n",
    "for i,d in enumerate(dirs):\n",
    "    # 파일 목록 읽어오기\n",
    "    files = os.listdir(dir_prefix + target_dir + '/' + d)\n",
    "    \n",
    "    for file in files:\n",
    "        # 각 파일을 읽어 들이기\n",
    "        f = open(dir_prefix + target_dir + '/' + d + '/' + '/' + file, 'r', encoding = 'utf-8')\n",
    "        raw = f.read()\n",
    "        \n",
    "        # 정규표현식을 사용해 불필요한 문자열을 제거한 다음 파일 내용을 출력\n",
    "        reg_raw = re.sub(r'[-\\'@#:/0-9a-zA-A<>!-\"*\\(\\)]', '', raw)\n",
    "        reg_raw = re.sub(r'[ ]+', ' ', reg_raw)\n",
    "        reg_raw = reg_raw.replace('\\n', ' ')\n",
    "        \n",
    "        # 형태소 분석을 거쳐 명사만 추출한 리스트를 생성\n",
    "        tokens = tokenizer.nouns(reg_raw)\n",
    "        \n",
    "        for token in tokens:\n",
    "            tmp1.append(token)\n",
    "            \n",
    "        tmp2 = ' '.join(tmp1)\n",
    "        x_ls.append(tmp2)\n",
    "        tmp1 = []\n",
    "        \n",
    "        # 기사 주제 레이블을 리스트에 저장\n",
    "        y_ls.append(i)\n",
    "        \n",
    "        # 파일 닫기\n",
    "        f.close()\n",
    "        \n",
    "        javax.servlet.ServletException: no fasoopackager in java.library.path\n",
    "            java -Djava.library.path=라이브러리 경로 클래스명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whM31f3KXHOD"
   },
   "outputs": [],
   "source": [
    "# 데이터프레임으로 변환해서 설명변수를 화면에 출력\n",
    "pd.DataFrame(x_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 기사로부터 추출한 단어를 출력\n",
    "print(x_ls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 목적변수를 화면에 출력\n",
    "print(y_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설명변수와 목적변수를 Numpy 배열로 변환\n",
    "x_array = np.array(x_ls)\n",
    "y_array = np.array(y_ls)\n",
    "\n",
    "# 단어 출현 횟수를 계수\n",
    "cntvec = CountVectorizer()\n",
    "x_cntvecs = cntvec.fit_transform(x_array)\n",
    "x_cntarray = x_cntvecs.toarray()\n",
    "\n",
    "# 데이터프레임으로 단어 출현 횟수 출력\n",
    "pd.DataFrame(x_cntarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어와 단어의 인덱스 표시\n",
    "for k, v in sorted(cntvec.vocabulary_.items(), key=lambda x:x[1]):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어의 TF-IDF 계산\n",
    "tfidf_vec = TfidfVectorizer(use_idf=True)\n",
    "x_tfidf_vecs = tfidf_vec.fit_transform(x_array)\n",
    "x_tfidf_array = x_tfidf_vecs.toarray()\n",
    "\n",
    "# 데이터프레임으로 변환해서 단어의 출현 횟수를 출력\n",
    "pd.DataFrame(x_tfidf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 훈련 데이터와 테스트 데이터로 분할\n",
    "train_X, test_X, train_Y, test_Y = model_selection.train_test_split(x_tfidf_array, y_array, test_size=0.2)\n",
    "\n",
    "# 데이터 건수 확인\n",
    "print(len(train_X))\n",
    "print(len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch 라이브러리 임포트\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 텐서 생성\n",
    "train_X = torch.from_numpy(train_X).float()\n",
    "train_Y = torch.from_numpy(train_Y).long()\n",
    "\n",
    "# 테스트 데이터 텐서 생성\n",
    "test_X = torch.from_numpy(test_X).float()\n",
    "test_Y = torch.from_numpy(test_Y).long()\n",
    "\n",
    "# 텐서로 변환한 데이터 건수 확인\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설명변수와 목적변수의 텐서를 합친다\n",
    "train = TensorDataset(train_X, train_Y)\n",
    "\n",
    "# 첫 번째 텐서 내용 확인\n",
    "print(train[0])\n",
    "\n",
    "# 미니배치 분할\n",
    "train_loader = DataLoader(train, batch_size = 100, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 구성\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init()\n",
    "        self.fc1 = nn.Linear(33572, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 128)\n",
    "        self.fc6 = nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "# 인스턴스 생성\n",
    "model = Net()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오차함수\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 최적화 기법 선택\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "# 학습\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # 분할한 데이터(미니배치)를 차례로 꺼내옴\n",
    "    for train_x, train_y = Variable(train_x), Variable(train_y):\n",
    "        # 경사 초기화\n",
    "        optimizer.zero_grad()\n",
    "        # 순전파 계산\n",
    "        output = model(train_x)\n",
    "        # 오차 계산\n",
    "        loss = criterion(output, train_y)\n",
    "        # 역전파 계산\n",
    "        loss.backward()\n",
    "        # 가중치 업데이트\n",
    "        optimizer.step()\n",
    "        # 오차 누적 계산\n",
    "        total_loss += loss.data\n",
    "        \n",
    "    # 각 에포크마다 누적 오차를 출력\n",
    "    print(epoch+1, total_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계산 그래프 구성\n",
    "test_x, test_y = Variable(test_X), Variable(test_Y)\n",
    "\n",
    "# 출력이 0 혹은 1이 되도록\n",
    "result = torch.max(model(test_x).data, 1)[1]\n",
    "\n",
    "# 모형의 정확도 계산\n",
    "accuracy = sum(test_y.cpu().data.numpy() == result.cpu().numpy()) / len(test_y.cpu().data.numpy())\n",
    "\n",
    "# 정확도 출력\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "뉴스데이터.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
